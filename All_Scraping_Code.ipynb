{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[1]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import requests\n", "import pandas as pd\n", "from bs4 import BeautifulSoup\n", "from geopy.geocoders import Nominatim"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["make function to check url rquest and parse url content ant retern soup"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_url(url):\n", "    #request url \n", "    url1=requests.get(str(url))\n", "    \n", "    #parse url content\n", "    soup=BeautifulSoup(url1.content,\"html.parser\")\n", "    \n", "    #retyrn soup\n", "    return soup \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["get url request "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["website_soup=get_url(\"https://www.semsarmasr.com\")\n", "#website_soup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["et function to extract number from the string using regex"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import re\n", "def return_num(price):\n", "  text=price.replace(\",\",\"\")\n", "  pattern=\"[0-9]+\"\n", "  price=re.findall(pattern,text)\n", "  return price"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Opensooq_scraping():\n", "    \n", "    df=pd.DataFrame(columns=[\"link\",\"bathrooms\",\"bedrooms\",\"city\",\"location1\",\"location2\",\"price\",\"size\",\"price_per_unit\",\"type\",\"unit\",\"lat\",\"lon\",\"view\",\"description\",\"web_name\",\"source\",\"rent_sale\"])\n", "    category_list=[7585,7891]\n", "    for category in category_list:\n", "        if category==7585:\n", "            page_num=1\n", "        else:\n", "            page_num=1\n", "      while (True):\n", "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n", "        url = (f\"https://eg.opensooq.com/ar/find?page={page_num}&PostSearch[categoryId]={category}\")\n", "        print(url)\n", "        result = requests.get(url, headers=headers)\n", "        src = result.content\n", "        soup = BeautifulSoup(src, \"lxml\")\n\n", "        #find apartment or every advertsment container------------------------------------------------------------------------------\n", "        apartments=soup.find(\"div\",{\"class\",\"pt-16\"}).find_all(\"div\",{\"class\":\"mb-32 relative\"})\n\n", "        #loop in every adverts to extract data----------------------------------------------------------------------------------------\n", "        links=[]\n", "        for apartment in apartments:\n", "          #find link of adverts -------------------------------------------------------------link-------------------------------------\n", "          link=\"https://eg.opensooq.com\"+apartment.find(\"a\").attrs[\"href\"]\n", "          print(f\"the link of adverts: {link}\")\n", "          links.append(link)\n", "          adverts_soup=get_url(link)\n", "          #find rent_sale----------------------------------------------------------------rent_sale------------------------------------------------------\n", "          if category==7585:\n", "            rent_sale=\"\u0644\u0644\u0628\u064a\u0639\"\n", "            print(f\"the rent_sale : {rent_sale}\")\n", "          else:\n", "            rent_sale=\"\u0644\u0644\u0625\u064a\u062c\u0627\u0631\"\n", "            print(f\"the rent_sale : {rent_sale}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["          #find unit----------------------------------------------------------------unit------------------------------------------------------\n", "          unit=apartment.find(\"div\",{\"class\":\"sc-265a2526-3 zoIKj postDet flex-1 flex flexSpaceBetween flexDirectionColumn\"}).find(\"div\",{\"class\":\"flex alignItems\"}).find(\"div\",{\"class\":\"me-8 category\"}).text.split(\"\u0641\u064a\")[0]\n", "          print(f\"the unit of apart : {unit}\")\n", "          #find descryption-----------------------------------------------------------descryption----------------------------------------------------------\n", "          descryptions=apartment.find(\"div\",{\"class\":\"sc-265a2526-3 zoIKj postDet flex-1 flex flexSpaceBetween flexDirectionColumn\"}).find(\"div\",{\"class\":\"flex flexSpaceBetween\"}).find(\"h2\").text\n", "          print(f\"the descryption is : {descryptions}\")\n", "          #find table which have data in adverts---------------------------------------------------------------table------------------------\n", "          try:\n", "            table=adverts_soup.find(\"div\",{\"class\":\"sc-23286f3d-0 cluGaS mt-32 mb-32 border radius-8 p-16\"}).find_all(\"div\",{\"class\":\"flex flexSpaceBetween flexWrap mt-8\"})\n", "          except:\n", "            pass\n", "          #intialize values of data ------------------------------------------------------------------------------------------------------------------------\n", "          bathrooms=None\n", "          bedrooms=None\n", "          city=None\n", "          location1=None\n", "          price=None\n", "          size=None\n", "          price_per_unit=None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["          #find price ------------------------------------------------------------price-------------------------------------------------------------\n", "          try:\n", "            price=adverts_soup.find(\"section\",{\"class\":\"sc-a8a57ece-2 cvAeHJ sideBar pb-16\"}).find(\"div\",{\"class\":\"sc-fab7f819-0 caVLzG radius-8\"}).find(\"span\").text\n", "            price=return_num(price)[0]\n", "            print(f\"the price is: {price}\")\n", "          except:\n", "            price=None\n", "            print(f\"the price is :{price}\")\n", "          #loop in table of content -------------------------------------------------------------------------------------------------------------------\n", "          for data in table:\n", "            #find bathrooms--------------------------------------------------------------------------------------bathrooms-------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"p\").text==\"\u0639\u062f\u062f \u0627\u0644\u062d\u0645\u0627\u0645\u0627\u062a\":\n", "                bathrooms=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"a\").text.replace(\"\u062d\u0645\u0651\u0627\u0645\u064a\u0646\",\"2\").replace(\"\u062d\u0645\u0651\u0627\u0645\u0627\u062a\",\"\").replace(\"\u062d\u0645\u0651\u0627\u0645\",\"1\").replace(\"+\",\"\")\n", "                print(f\"the num_bathroom is: {bathrooms}\")\n", "            except:\n", "              pass"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            #find bedrooms------------------------------------------------------------------------------------------------bedrooms-----------------------------------------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"p\").text==\"\u0639\u062f\u062f \u0627\u0644\u063a\u0631\u0641\":\n", "                bedrooms=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"a\").text.replace(\"\u063a\u0631\u0641\u0629 \u0646\u0648\u0645\",\"1\").replace(\"\u063a\u0631\u0641 \u0646\u0648\u0645\",\"\").replace(\"\u063a\u0631\u0641\u062a\u0627 \u0646\u0648\u0645\",\"\").replace(\"\u063a\u0631\u0641\",\"\").replace(\"+\",\"\")\n", "                #bedrooms=return_num(bedrooms)\n", "                print(f\"the num_bedrooms is: {bedrooms}\")\n", "            except:\n", "              pass\n\n", "            #find city------------------------------------city------------------------------------------------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"p\").text==\"\u0627\u0644\u0645\u062f\u064a\u0646\u0629\":\n", "                city=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"a\").text\n", "                #bedrooms=return_num(bedrooms)\n", "                print(f\"the city is: {city}\")\n", "            except:\n", "              pass\n\n", "            #find size------------------------------------size-------------------------------------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0623\u0631\u0636\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0623\u0631\u0636\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0628\u0646\u0627\u0621\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0628\u0646\u0627\u0621\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[0].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "            except:\n", "              pass\n\n", "            #find size--------------------------------------------------------------------size-------------------------------------------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0628\u0646\u0627\u0621\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0623\u0631\u0636\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[1].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0628\u0646\u0627\u0621\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "              elif data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"p\").text==\"\u0645\u0633\u0627\u062d\u0629 \u0627\u0644\u0623\u0631\u0636\":\n", "                size=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems\"})[0].find(\"a\").text\n", "                size=return_num(size)[0]\n", "                print(f\"the size is: {size}\")\n", "            except:\n", "              pass\n\n", "            #find location1------------------------------------location1----------------------------------------------------------------------------------------\n", "            try:\n", "              if data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"p\").text==\"\u0627\u0644\u062d\u064a / \u0627\u0644\u0645\u0646\u0637\u0642\u0629\":\n", "                location1=data.find_all(\"div\",{\"class\":\"sc-23286f3d-1 hhPCva flex flexSpaceBetween alignItems lightGrayBg\"})[1].find(\"a\").text\n", "                #bedrooms=return_num(bedrooms)\n", "                print(f\"the location1 is: {location1}\")\n", "            except:\n", "              pass\n", "          #find price_per_unit------------------------------------------------------------------price_per_unite---------------------------------------------\n", "          try:\n", "            price_per_unit=int(price)/int(size)\n", "            print(f\"the price_per_unit is : {price_per_unit}\")\n", "          except:\n", "            price_per_unit=None\n", "            print(f\"the price_per_unit is : {price_per_unit}\")      "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["            #find bathrooms---------------------------------------------------------------------------------------bathrooms---------------------------\n", "          df=df.append({\"link\":link,\n", "                          \"bathrooms\":bathrooms,\n", "                        \"bedrooms\":bedrooms,\n", "                        \"city\":city,\n", "                        \"location1\":location1,\n", "                        \"location2\":location1,\n", "                        \"price\":price,\n", "                        \"size\":size,\n", "                        \"price_per_unit\":price_per_unit,\n", "                        \"unit\":unit,\n", "                        \"web_name\":\"OpenSooq\",\n", "                        \"source\":\"scraping\",\n", "                        \"description\":descryptions,\n", "                        \"rent_sale\":rent_sale\n", "                        },ignore_index=True)\n", "          df.to_csv(\"sooqelmftoh_N2.csv\",encoding=\"utf-8-sig\",index=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        next_page=soup.find(\"div\",{\"id\":\"pagination\"}).find(\"a\",{\"class\":\"sc-c272c79-4 gbyEnK grayBg blueHoverBg pagerItem flex alignItems justifyContent radius-8\"})\n", "        print(f\"the next page is: {next_page}\")\n", "        if next_page !=None:\n", "          print(\"i am finding the next page !!\")\n", "          page_num+=1\n", "        else:\n", "          print(\" ohhh idont find the next page!!!!\")\n", "          break\n", "            \n", "            "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Opensooq_clean():\n", "    df=pd.read_csv(\"sooqelmftoh_N2.csv\")\n", "    #clean type--------------------------------------------------\n", "    df.loc[df.unit ==\"\u0634\u0642\u0642 \u0644\u0644\u0628\u064a\u0639 \",'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit =='\u0641\u0644\u0644 - \u0642\u0635\u0648\u0631 \u0644\u0644\u0628\u064a\u0639 ', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit =='\u0639\u0645\u0627\u0631\u0627\u062a \u0644\u0644\u0628\u064a\u0639 ','type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u0628\u064a\u0648\u062a - \u0645\u0646\u0627\u0632\u0644 \u0644\u0644\u0628\u064a\u0639 \",'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u0639\u0642\u0627\u0631\u0627\u062a \u0623\u062c\u0646\u0628\u064a\u0629 \u0644\u0644\u0628\u064a\u0639 \", 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0634\u0642\u0642 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0641\u0644\u0644 - \u0642\u0635\u0648\u0631 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0628\u064a\u0648\u062a - \u0645\u0646\u0627\u0632\u0644 \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0639\u0645\u0627\u0631\u0627\u062a \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u063a\u0631\u0641 \u0648\u0645\u0634\u0627\u0631\u0643\u0629 \u0633\u0643\u0646 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0628\u064a\u0639 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0628\u064a\u0639 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639 \u0648\u0634\u0627\u0644\u064a\u0647\u0627\u062a \u0644\u0644\u0628\u064a\u0639 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0627\u064a\u062c\u0627\u0631\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u062a\u062c\u0627\u0631\u064a \u0644\u0644\u0628\u064a\u0639 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0634\u0642\u0642 \u0648\u0623\u062c\u0646\u062d\u0629 \u0641\u0646\u062f\u0642\u064a\u0629 \",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a \u0644\u0644\u0628\u064a\u0639 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639 \u0648\u0634\u0627\u0644\u064a\u0647\u0627\u062a \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #clean unit------------------------------------------\n", "    df.loc[df.unit ==\"\u0634\u0642\u0642 \u0644\u0644\u0628\u064a\u0639 \",'unit'] = \"\u0634\u0642\u0629\"\n", "    df.loc[df.unit =='\u0641\u0644\u0644 - \u0642\u0635\u0648\u0631 \u0644\u0644\u0628\u064a\u0639 ', 'unit'] = \"\u0641\u064a\u0644\u0627\"\n", "    df.loc[df.unit =='\u0639\u0645\u0627\u0631\u0627\u062a \u0644\u0644\u0628\u064a\u0639 ','unit'] = \"\u0639\u0645\u0627\u0631\u0629\"\n", "    df.loc[df.unit ==\"\u0628\u064a\u0648\u062a - \u0645\u0646\u0627\u0632\u0644 \u0644\u0644\u0628\u064a\u0639 \",'unit'] = \"\u0645\u0646\u0632\u0644\"\n", "    df.loc[df.unit ==\"\u0639\u0642\u0627\u0631\u0627\u062a \u0623\u062c\u0646\u0628\u064a\u0629 \u0644\u0644\u0628\u064a\u0639 \", 'unit'] = \"\u0639\u0642\u0627\u0631\"\n", "    df.loc[df.unit==\"\u0634\u0642\u0642 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0634\u0642\u0629\"\n", "    df.loc[df.unit==\"\u0641\u0644\u0644 - \u0642\u0635\u0648\u0631 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0641\u064a\u0644\u0627\"\n", "    df.loc[df.unit==\"\u0628\u064a\u0648\u062a - \u0645\u0646\u0627\u0632\u0644 \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0645\u0646\u0632\u0644\"\n", "    df.loc[df.unit==\"\u0639\u0645\u0627\u0631\u0627\u062a \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0639\u0645\u0627\u0631\u0629\"\n", "    df.loc[df.unit==\"\u063a\u0631\u0641 \u0648\u0645\u0634\u0627\u0631\u0643\u0629 \u0633\u0643\u0646 \",\"unit\"]=\"\u063a\u0631\u0641\u0629\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0628\u064a\u0639 \",\"unit\"]=\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639 \u0648\u0634\u0627\u0644\u064a\u0647\u0627\u062a \u0644\u0644\u0628\u064a\u0639 \",\"unit\"]=\"\u0645\u0632\u0627\u0631\u0639\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0627\u064a\u062c\u0627\u0631\",\"unit\"]=\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629\"\n", "    df.loc[df.unit==\"\u062a\u062c\u0627\u0631\u064a \u0644\u0644\u0628\u064a\u0639 \",\"unit\"]=\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629\"\n", "    df.loc[df.unit==\"\u0634\u0642\u0642 \u0648\u0623\u062c\u0646\u062d\u0629 \u0641\u0646\u062f\u0642\u064a\u0629 \",\"unit\"]=\"\u0634\u0642\u0629\"\n", "    df.loc[df.unit==\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629 \u0644\u0644\u0627\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0623\u0628\u0646\u064a\u0629 \u062a\u062c\u0627\u0631\u064a\u0629\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a \u0644\u0644\u0628\u064a\u0639 \",\"unit\"]=\"\u0627\u0631\u0636\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639 \u0648\u0634\u0627\u0644\u064a\u0647\u0627\u062a \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0645\u0632\u0631\u0639\u0629\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a \u0644\u0644\u0625\u064a\u062c\u0627\u0631 \",\"unit\"]=\"\u0627\u0631\u0636\"\n", "    \n", "    #cleant rent_sale--------------------------------------\n", "    df.loc[df.rent_sale==\"\u0644\u0644\u0628\u064a\u0639\",\"rent_sale\"]=\"\u0628\u064a\u0639\"\n", "    df.loc[df.rent_sale==\"\u0644\u0644\u0625\u064a\u062c\u0627\u0631\",\"rent_sale\"]=\"\u0627\u064a\u062c\u0627\u0631\"\n", "    df.loc[df.rent_sale==\"\u0645\u0637\u0644\u0648\u0628 \u0644\u0644\u0634\u0631\u0627\u0621\",\"rent_sale\"]=\"\u0628\u064a\u0639\"\n", "    df.loc[df.rent_sale==\"\u0625\u064a\u062c\u0627\u0631 \u0642\u0627\u0646\u0648\u0646 \u0642\u062f\u064a\u0645\",\"rent_sale\"]=\"\u0627\u064a\u062c\u0627\u0631\"\n", "    \n", "    \n", "    df[\"description\"]=df[\"description\"].str.replace('\\n', '')\n", "    df[\"description\"]=df[\"description\"].str.strip()\n", "    \n", "    \n", "    df.loc[df.location1==\"\u0623\u062e\u0631\u0649\",\"location1\"]=df[\"city\"]\n", "    df.loc[df.location2==\"\u0623\u062e\u0631\u0649\",\"location2\"]=df[\"city\"]\n", "    \n", "    \n", "    #clean numbathrooms\n", "    #clean bathrooms\n", "    df.loc[df.bathrooms==\"\u0662\",\"bathrooms\"]=\"2\"\n", "    df.loc[df.bathrooms=='\u0664 ',\"bathrooms\"]=\"4\"\n", "    df.loc[df.bathrooms=='\u0663 ',\"bathrooms\"]=\"3\"\n", "    df.loc[df.bathrooms=='\u0665 ',\"bathrooms\"]=\"5\"\n\n", "    #clean bedrooms\n", "    df.loc[df.bedrooms=='\u0666 ',\"bedrooms\"]=\"6\"\n", "    df.loc[df.bedrooms=='\u0664 ',\"bedrooms\"]=\"4\"\n", "    df.loc[df.bedrooms=='\u0663 ',\"bedrooms\"]=\"3\"\n", "    df.loc[df.bedrooms=='\u0662 ',\"bedrooms\"]=\"2\"\n", "    df.loc[df.bedrooms=='\u0665 ',\"bedrooms\"]=\"5\"\n", "    df.loc[df.bedrooms=='\u0633\u062a\u0648\u062f\u064a\u0648',\"bedrooms\"]=None\n", "    \n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639\",\"unit\"]=\"\u0645\u0632\u0631\u0639\u0629\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #find lat lon --------------------------------------------------------\n", "    from geopy.geocoders import Nominatim"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    geolocator = Nominatim(user_agent=\"myApp\")\n", "    for i in df.index:\n", "        print(i)\n", "        try:\n", "            #tries fetch address from geopy\n", "            location = geolocator.geocode(df[\"location1\"][i]+\"-\"+df['city'][i]+\"-\"+\"\u0645\u0635\u0631\")\n", "            print(location)\n\n", "            #append lat/long to column using dataframe location\n", "            df['lat'][i] = location.latitude\n", "            df['lon'][i]= location.longitude\n", "        #catches exception for the case where no value is returned\n", "        #appends null value to column\n", "        except:\n", "            df['lat'][i] = \"\"\n", "            df['lon'][i] = \"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    print(df[\"lat\"])\n", "    \n", "    sooq_after=df.to_csv(\"Opeensooq_Clean2.csv\",encoding=\"utf-8-sig\",index=False)\n", "    \n", "    return sooq_after"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Semsar_scraping():\n", "    \n", "    category_list=[765,844,766,898,767,0,770,768,876,769,771]\n", "    df=pd.DataFrame(columns=[\"link\",\"bathrooms\",\"bedrooms\",\"city\",\"location1\",\"location2\",\"price\",\"size\",\"price_per_unit\",\"type\",\"unit\",\"lat\",\"lon\",\"view\",\"description\",\"web_name\",\"source\",\"rent_sale\"])\n", "    links=[]\n", "    for category in category_list:\n", "      print(f\"the category is: {category}\")\n", "      if category==765:\n", "          page_num=1\n", "      else:\n", "        page_num=1\n", "      while (True):\n", "        print(f\"the num_page: {page_num}\")\n", "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n", "        url = (f\"https://www.semsarmasr.com/3akarat?title=%CF%E6%C8%E1%DF%D3%20%E1%E1%C8%ED%DA%20%DD%ED%20%E3%D5%D1&r=70&g=0&a=0&cid={category}&p={page_num}\")\n", "        print(url)\n", "        result = requests.get(url, headers=headers)\n", "        src = result.content\n", "        soup = BeautifulSoup(src, \"lxml\")\n", "        try:\n", "          links_in_page = soup.find(\"div\",{\"id\":\"table\"}).find_all(\"div\",{\"id\":\"row\"})\n", "        except:\n", "          pass\n", "          #print(links_in_page)\n\n", "        #looop in each page and find links of each adevertsment in page and parse it and extract data from it \n", "        for apart in links_in_page:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["          l = apart.find(\"a\").attrs['href'] #find link of each advertsment\n", "          print(f\"the link of the adves :{l}\")\n", "          adverts_soup=get_url(l)  #get soup of each link \n", "          #find table of content in each advertsment-------------------find tabel--------------------------\n", "          try:\n", "            table= adverts_soup.find(\"div\",{\"class\":\"Row\"}).find(\"div\",{\"class\":\"AdDetContainer ListDesStyle AdDetailsStyle\"}).find(\"div\",{\"class\":\"table\"}).find_all(\"div\",{\"id\":\"DetRrow\"})\n", "          except:\n", "            pass\n", "          # intialize the values of this features -----------------------------------------------------------------\n", "          price=None\n", "          area=None  \n", "          num_room=None\n", "          num_bathroom=None\n", "          rent_sale=None\n", "          unit=None\n", "          location1=None\n", "          city=None\n", "          price_per_unite=None\n", "          # loop in table to extract data---------------------table of data-----------------------------------------------------------\n", "          for data in table:\n", "            #find price------------------------price--------------------------------------------------------------------------------\n", "            if data.find_all(\"div\")[0].text==\"\u0627\u0644\u0633\u0639\u0631: \":\n", "              price=data.find_all(\"div\")[1].find(\"strong\").text\n", "              price=return_num(price)\n", "              try:\n", "                price=price[0]\n", "                print(\"the price is: \"+price)\n", "              except:\n", "                price=None\n", "                print(f\"the price is :{price}\")\n", "            #find size---------------------------size-----------------------------------------------------------------------------------\n", "            elif data.find_all(\"div\")[0].text==\"\u0627\u0644\u0645\u0633\u0627\u062d\u0629: \":\n", "              area=data.find_all(\"div\")[1].text\n", "              area=return_num(area)\n", "              area=area[0]\n", "              print(\"the area is : \"+area)\n\n", "            #find numbers of bedroom----------------num_bedrooms-------------------------------------------------------------------------\n", "            elif data.find_all(\"div\")[0].text==\"\u0639\u062f\u062f \u0627\u0644\u063a\u0631\u0641: \":\n", "              num_room=data.find_all(\"div\")[1].text\n", "              print(\"the num_room is: \"+num_room)\n\n", "            #find number of bathrooms----------------num_bathroom-------------------------------------------------------------------------\n", "            elif data.find_all(\"div\")[0].text==\"\u0639\u062f\u062f \u0627\u0644\u062d\u0645\u0627\u0645\u0627\u062a: \":\n", "              num_bathroom=data.find_all(\"div\")[1].text\n", "              print(\"the number of batheroom is: \"+num_bathroom)\n\n", "            #find type of category------------------rent_sale-------------------------------------------------------------------------------\n", "            elif data.find_all(\"div\")[0].text==\"\u0627\u0644\u063a\u0631\u0636: \":\n", "              rent_sale=data.find_all(\"div\")[1].text\n", "              print(\"the rent_sale : \"+rent_sale)\n", "            #find unit--------------------------------------------------------------------unit-----------------------------------------------\n", "            elif data.find_all(\"div\")[0].text==\"\u0627\u0644\u0642\u0633\u0645: \":\n", "              unit=data.find_all(\"div\")[1].find(\"a\").find(\"span\").text.split(\" \")[0]\n", "              print(\"the unite of category is: \"+unit)\n", "          #find price per unite-------------------------------------------------------------------------------------price_per_unite-------------------------------------------\n", "          try:\n", "            price_per_unite=int(float(price)/float(area))\n", "            print(f\"the price per unite is :{int(price_per_unite)}\")\n", "          except:\n", "            price_per_unite=None\n", "          #find location1------------------------------------------------------location1----------------------------------------\n", "          try:\n", "            city=apart.find_all(\"div\",{\"class\":\"Intcell\"})[1].find(\"div\",{\"class\":\"AdLocation\"}).text.split(\"-\")[1]\n", "            print(\"the city is : \"+city)\n", "          except:\n", "            pass\n", "          #find location2---------------------------------------------------------------------locatuion2-------------------------------------\n", "          location2=apart.find_all(\"div\",{\"class\":\"Intcell\"})[1].find(\"div\",{\"class\":\"AdLocation\"}).text\n", "          print(\"the location2 is: \"+location2)\n", "          #find city------------------------------------------------------city---------------------------------------------------\n", "          try:\n", "            location1=apart.find_all(\"div\",{\"class\":\"Intcell\"})[1].find(\"div\",{\"class\":\"AdLocation\"}).text.split(\"-\")[0]\n", "            print(\"the ;ocation1 is: \"+location1)\n", "          except:\n", "           pass\n", "          #find descryption--------------------------descryption-----------------------------------------------------   \n", "          try:\n", "            description=adverts_soup.find(\"div\",{\"class\":\"PageTitlesStyle\"}).find(\"a\").text.replace(\"\u0625\u0639\u0644\u0627\u0646 \u0645\u064f\u0645\u064a\u0632\",\"\") #find descryption \n", "            print(\"the description is: \"+description)\n", "          except:\n", "            pass\n", "          print(\"------------------------------------------------>\")\n", "          #append data on data frame------------------------------------------------------------------------------------------------\n", "          df=df.append({\"link\":l,\n", "                         \"bathrooms\":num_bathroom,\n", "                         \"bedrooms\":num_room,\n", "                         \"price\":price,\n", "                         \"size\":area,\n", "                         \"location1\":location1,\n", "                         \"location2\":location1,\n", "                         \"city\":city,\n", "                         \"unit\":unit,\n", "                         \"rent_sale\":rent_sale,\n", "                         \"description\":description,\n", "                         \"web_name\":\"semsar\",\n", "                         \"source\":\"scraping\",\n", "                        \"price_per_unit\":price_per_unite\n", "                         },ignore_index=True)\n", "                        #convert data frame into csv file-------------------------------------------------------------------------\n", "          out=df.to_csv(\"Semsar3.csv\",encoding=\"utf-8-sig\",index=False)\n", "          links.append(l)\n", "        print(len(links))\n\n", "        #find next page to break while loop if the condition is not true ----------------------------------------------\n", "        try:\n", "          next_page_link = soup.find(\"div\",{\"id\":\"paging\"}).find(\"a\",{\"class\":\"notactive\"})\n", "          print(next_page_link)\n", "        except:\n", "          next_page_link = soup.find(\"div\",{\"id\":\"paging\"}).find(\"a\",{\"class\":\"notactive\"})\n", "          print(next_page_link) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        if next_page_link==None or next_page_link.text ==\"\u276e\u276e \u0627\u0644\u0623\u0648\u0644\" :\n", "            print(\"donesssssssssssss\")\n", "            page_num += 1\n", "        else:\n", "            print(\"-----------End the category is  done---------\")\n", "            break\n", "    return out\n", "            "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def Semsar_cleaning():\n", "    \n", "    df=pd.read_csv(\"Semsar3.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #find lat and lond of location\n", "    from geopy.geocoders import Nominatim"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    geolocator = Nominatim(user_agent=\"myApp\")\n", "    for i in df.index:\n", "        print(i)\n", "        try:\n", "            #tries fetch address from geopy\n", "            location = geolocator.geocode(df[\"location1\"][i]+\"-\"+df['city'][i]+\"-\"+\"\u0645\u0635\u0631\")\n", "            print(location)\n\n", "            #append lat/long to column using dataframe location\n", "            df['lat'][i] = location.latitude\n", "            df['lon'][i]= location.longitude\n", "        #catches exception for the case where no value is returned\n", "        #appends null value to column\n", "        except:\n", "            df['lat'][i] = \"\"\n", "            df['lon'][i] = \"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    print(df[\"lat\"])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    #cleant rent_sale--------------------------------------\n", "    df.loc[df.rent_sale==\"\u0644\u0644\u0628\u064a\u0639\",\"rent_sale\"]=\"\u0628\u064a\u0639\"\n", "    df.loc[df.rent_sale==\"\u0644\u0644\u0625\u064a\u062c\u0627\u0631\",\"rent_sale\"]=\"\u0627\u064a\u062c\u0627\u0631\"\n", "    df.loc[df.rent_sale==\"\u0645\u0637\u0644\u0648\u0628 \u0644\u0644\u0634\u0631\u0627\u0621\",\"rent_sale\"]=\"\u0628\u064a\u0639\"\n", "    df.loc[df.rent_sale==\"\u0625\u064a\u062c\u0627\u0631 \u0642\u0627\u0646\u0648\u0646 \u0642\u062f\u064a\u0645\",\"rent_sale\"]=\"\u0627\u064a\u062c\u0627\u0631\"\n", "    \n", "    \n", "    #clean type-----------------------------------\n", "    df.loc[df.unit ==\"\u0634\u0642\u0642\",'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u0641\u0644\u0644\", 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u062f\u0648\u0628\u0644\u0643\u0633\",'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u0634\u0627\u0644\u064a\u0647\u0627\u062a\",'type'] = \"\u0633\u0643\u0646\u0649\"\n", "    df.loc[df.unit ==\"\u0641\u0646\u0627\u062f\u0642\", 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit ==\"\u0645\u0628\u0627\u0646\u0649\", 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0628\u0646\u0627\u064a\u0627\u062a\",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0639\u0645\u0627\u0631\u0627\u062a\",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u063a\u0631\u0641\",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u063a\u0631\u0641\u0629\",\"type\"]=\"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0643\u0627\u062a\u0628\",\"type\"]=\"\u0627\u062f\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u062d\u0644\u0627\u062a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u062c\u0631\u0627\u062c\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0635\u064a\u062f\u0644\u064a\u0627\u062a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0639\u064a\u0627\u062f\u0627\u062a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0642\u0627\u0639\u0627\u062a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u062e\u0627\u0632\u0646\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0631\u0627\u0643\u0632\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0633\u062a\u0634\u0641\u064a\u0627\u062a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u063a\u0633\u0644\u0629\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0637\u0627\u0639\u0645\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0635\u0627\u0646\u0639\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u062d\u0637\u0629\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u062f\u0627\u0631\u0633\",\"type\"]=\"\u062a\u0639\u0644\u064a\u0645\u0649\"\n", "    df.loc[df.unit==\"\u062d\u0636\u0627\u0646\u0627\u062a\",\"type\"]=\"\u062a\u0639\u0644\u064a\u0645\u0649\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a\",\"type\"]=\"\u0632\u0631\u0627\u0639\u0649\"\n", "    df.loc[df.unit==\"\u062d\u062f\u0627\u0626\u0642\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0633\u062a\u0648\u062f\u064a\u0648\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit==\"\u0645\u0628\u0627\u0646\u064a\",\"type\"]=\"\u062a\u062c\u0627\u0631\u064a\"\n", "    \n", "    \n", "    #clean unit -------------------------------------\n", "    df.loc[df.unit ==\"\u0634\u0642\u0642\",'unit'] = \"\u0634\u0642\u0629\"\n", "    df.loc[df.unit =='\u0641\u0644\u0644', 'unit'] = \"\u0641\u064a\u0644\u0627\"\n", "    df.loc[df.unit =='\u0639\u0645\u0627\u0631\u0627\u062a','unit'] = \"\u0639\u0645\u0627\u0631\u0629\"\n", "    df.loc[df.unit ==\"\u0628\u064a\u0648\u062a - \u0645\u0646\u0627\u0632\u0644 \u0644\u0644\u0628\u064a\u0639 \",'unit'] = \"\u0645\u0646\u0632\u0644\"\n", "    df.loc[df.unit ==\"\u0639\u0642\u0627\u0631\u0627\u062a\", 'unit'] = \"\u0639\u0642\u0627\u0631\"\n", "    df.loc[df.unit==\"\u0645\u0632\u0627\u0631\u0639\",\"unit\"]=\"\u0645\u0632\u0631\u0639\u0629\"\n", "    df.loc[df.unit==\"\u0639\u0645\u0627\u0631\u0627\u062a\",\"unit\"]=\"\u0639\u0645\u0627\u0631\u0629\"\n", "    df.loc[df.unit==\"\u0623\u0631\u0627\u0636\u064a\",\"unit\"]=\"\u0627\u0631\u0636\"\n", "    df.loc[df.unit==\"\u062f\u0648\u0628\u0644\u0643\u0633\",\"unit\"]=\"\u062f\u0648\u0628\u0644\u0643\u0633\"\n", "    df.loc[df.unit==\"\u0634\u0627\u0644\u064a\u0647\u0627\u062a\",\"unit\"]=\"\u0634\u0627\u0644\u064a\u0647\"\n", "    df.loc[df.unit==\"\u0645\u0643\u0627\u062a\u0628\",\"unit\"]=\"\u0645\u0643\u062a\u0628\"\n", "    df.loc[df.unit==\"\u0645\u062d\u0644\u0627\u062a\",\"unit\"]=\"\u0645\u062d\u0644\"\n", "    df.loc[df.unit==\"\u0635\u064a\u062f\u0644\u064a\u0627\u062a\",\"unit\"]=\"\u0635\u064a\u062f\u0644\u064a\u0629\"\n", "    df.loc[df.unit==\"\u062d\u0636\u0627\u0646\u0627\u062a\",\"unit\"]=\"\u062d\u0636\u0627\u0646\u0629\"\n", "    df.loc[df.unit==\"\u0645\u0637\u0627\u0639\u0645\",\"unit\"]=\"\u0645\u0637\u0639\u0645 \u0648 \u0643\u0627\u0641\u064a\u0647\"\n", "    df.loc[df.unit==\"\u0641\u0646\u0627\u062f\u0642\",\"unit\"]=\"\u0641\u0646\u062f\u0642\"\n", "    df.loc[df.unit==\"\u0633\u062a\u0648\u062f\u064a\u0648\",\"unit\"]=\"\u0633\u062a\u0648\u062f\u064a\u0648\"\n", "    df.loc[df.unit==\"\u0645\u0635\u0627\u0646\u0639\",\"unit\"]=\"\u0645\u0635\u0646\u0639\"\n", "    df.loc[df.unit==\"\u0639\u064a\u0627\u062f\u0627\u062a\",\"unit\"]=\"\u0639\u064a\u0627\u062f\u0629\"\n", "    df.loc[df.unit==\"\u0645\u0633\u062a\u0634\u0641\u064a\u0627\u062a\",\"unit\"]=\"\u0645\u0633\u062a\u0634\u0641\u0649\"\n", "    df.loc[df.unit==\"\u0645\u062e\u0627\u0632\u0646\",\"unit\"]=\"\u0645\u062e\u0632\u0646\"\n", "    df.loc[df.unit==\"\u0645\u0631\u0627\u0643\u0632\",\"unit\"]=\"\u0645\u0631\u0643\u0632\"\n", "    df.loc[df.unit==\"\u063a\u0631\u0641\",\"unit\"]=\"\u063a\u0631\u0641\u0629\"\n", "    df.loc[df.unit==\"\u0645\u0628\u0627\u0646\u064a\",\"unit\"]=\"\u0645\u0628\u0646\u0649\"\n", "    df.loc[df.unit==\"\u0642\u0627\u0639\u0627\u062a\",\"unit\"]=\"\u0642\u0627\u0639\u0629\"\n", "    df.loc[df.unit==\"\u0645\u062f\u0627\u0631\u0633\",\"unit\"]=\"\u0645\u062f\u0631\u0633\u0629\"\n", "    df.loc[df.unit==\"\u062d\u062f\u0627\u0626\u0642\",\"unit\"]=\"\u062d\u062f\u064a\u0642\u0629\"\n", "    \n", "    \n", "    df.to_csv(\"Semsar_clean.csv\",encoding=\"utf-8-sig\",index=False)\n", "    \n", "    \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tageer_scraping():\n", "    lst=[]                  #list of data\n", "        #list of categories\n", "    category_list=['flats-for-rent','shkk-k-non-kdym','furnished-flats','rooms-for-rent','villas-for-rent','business-rent-4','commercial-properties-4-rent','car-rental']\n", "        #list of all links\n", "    links=[]\n", "    #categories loop\n", "    for category in category_list:\n", "        #counter of pages\n", "        page_num=1   \n", "        while (True) :\n", "            #the headers of the web \n", "            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n", "            #link of all pages of the website\n", "            url=f'https://tageer.net/{category}?page={page_num}'\n", "            print(url)\n", "            #connecting to the web\n", "            result = requests.get(url, headers=headers)\n", "            src = result.content\n", "            soup = BeautifulSoup(src, \"lxml\")\n", "            #scraping main tag\n", "            links_in_page = soup.find_all('div',{'class':'caption pull-left'})\n", "            #scraping the link for each main tag\n", "            for i in links_in_page:\n", "                l = i.find(\"a\").attrs['href']                        \n", "                links.append(l)                         #adding link to list\n", "            next_page_link = soup.find(\"li\", {'class':\"disabled\"})                  #scraping the condition\n", "            if next_page_link != None:            \n", "                print('done')\n", "                break\n", "            else:\n", "                page_num+=1\n", "    #scraping each link\n", "    for link in links:\n", "        d={}\n", "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'}\n", "        result1 = requests.get(link, headers=headers)\n", "        src1= result1.content\n", "        soup1= BeautifulSoup(src1, \"lxml\")\n", "        d['link']=link\n", "        print(link)\n", "        d['web_name']='tageer'\n", "        print('tageer')\n", "        try:\n", "            d['price']=soup1.find('div',{'class':'col-xs-12 col-sm-2 price price-curry'}).text.replace('EGP','').strip()\n", "            print(soup1.find('div',{'class':'col-xs-12 col-sm-2 price price-curry'}).text.replace('EGP','').strip())\n", "        except:\n", "            d['price']=None\n", "        try:\n", "            d['unit']=soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[3].text.replace('|','').strip()\n", "            print(soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[3].text.replace('|','').strip())\n", "        except:\n", "            d['unit']=None\n", "        try:\n", "            d['location1']=soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[1].text.replace('|','').strip()\n", "            print(soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[1].text.replace('|','').strip())\n", "        except:\n", "            d['location1']=None\n", "        try:\n", "            d['location2']=soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[2].text.replace('|','').partition('\u060c')[0].strip()\n", "            print(soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[2].text.replace('|','').partition('\u060c')[0].strip())\n", "        except:\n", "            d['location2']=None\n", "        try:\n", "            d['city']=soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[2].text.replace('|','').partition('\u060c')[2].strip()\n", "            print(soup1.find('ul',{'class':'breadcrumb'}).find_all('a')[2].text.replace('|','').partition('\u060c')[2].strip())\n", "        except:\n", "            d['city']=None\n", "        try:\n", "            d['description']=soup1.find('div',{'class':'tab-pane fade active in'}).find('p').text.strip()\n", "            print(soup1.find('div',{'class':'tab-pane fade active in'}).find('p').text.strip())\n", "        except:\n", "            d['description']=None    \n", "        d['source']='scrape'\n", "        print('scrape')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        try:\n", "            table2=soup1.find('table',{'class':'table table-striped'})        #scraping the table for each link\n", "            table=table2.find_all('tr')\n", "            for i in table:\n", "                try:\n", "                    if i.find_all('td')[0].text=='\u0627\u0644\u0645\u0633\u0627\u062d\u0629 \u0628\u0627\u0644\u0645\u062a\u0631':\n", "                        d['size']=i.find_all('td')[1].text\n", "                        print(i.find_all('td')[1].text)\n", "                except:\n", "                    d['size']=None\n", "                    print(None)\n", "            for x in table:\n", "                try:\n", "                    if x.find_all('td')[0].text=='\u0639\u062f\u062f \u0627\u0644\u063a\u0631\u0641':\n", "                        d['bedrooms']=x.find_all('td')[1].text\n", "                        print(x.find_all('td')[1].text)\n", "                except:\n", "                    d['bedrooms']=None\n", "                    print(None)\n", "            for y in table:\n", "                try:\n", "                    if y.find_all('td')[0].text=='\u0639\u062f\u062f \u0627\u0644\u062d\u0645\u0627\u0645\u0627\u062a':\n", "                        d['bathrooms']=y.find_all('td')[1].text\n", "                        print(y.find_all('td')[1].text)\n", "                except:\n", "                    d['bathrooms']=None\n", "                    print(None)\n", "        except:\n", "            pass\n", "        #adding dictionary to list\n", "        lst.append(d)\n", "        df=pd.DataFrame(lst)#exporting data to csv file\n", "        df.to_csv(r'Tageer2.csv',encoding ='utf-8-sig',index=False)\n", "            "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tageer_cleaning():\n", "    df=pd.read_csv(r'Tageer2.csv',index_col=False)\n", "    df['all-location'] = df['location2'] + \",\" +  df['city']+','+'\u0645\u0635\u0631'\n", "    df['lat'] = \"\"\n", "    df['lon'] = \"\"\n", "    geolocator = Nominatim(user_agent=\"myApp\")\n", "    for i in df.index:\n", "        try:\n", "            #tries fetch address from geopy\n", "            location = geolocator.geocode(df['all-location'][i])\n\n", "            #append lat/long to column using dataframe location\n", "            df.loc[i,'lat'] = location.latitude\n", "            df.loc[i,'lon'] = location.longitude\n", "        except:\n", "            #catches exception for the case where no value is returned\n", "            #appends null value to column\n", "            df.loc[i,'lat'] = \"\"\n", "            df.loc[i,'lon'] = \"\"\n", "    \n", "    del df['all-location']\n", "    df['rent_sale']='\u0627\u064a\u062c\u0627\u0631'\n", "    df['price_per_unit']=''\n", "    df['type']=''\n", "    df['view']=''\n", "    df=df.dropna(subset=['unit'])\n", "    df=df.dropna(subset=['city'])\n", "    df=df.dropna(subset=['price'])\n", "    df=df.dropna(subset=['size'])\n", "    df=df.dropna(subset=['lon'])\n", "    #converting data type\n", "    df['price']=df['price'].str.replace(',','')\n", "    df['price']=df['price'].astype('float')\n", "    df['size']=df['size'].astype('float')\n", "    #replceing with correct values\n", "    df.bedrooms=df.bedrooms.replace({'\u0665':'5','\u0664':'4','\u0667':'7','\u0666':'6','\u0669':'9','\u0668':'8','\u0662':'2','\u0661':'1','\u0661\u0660':'10','\u0663':'3','\u0661\u0660+':'10','10+':'10'})\n", "#converting data type\n", "    df.bedrooms=df.bedrooms.astype('float')\n", "    df.bathrooms=df.bathrooms.replace({'\u0665':'5','\u0664':'4','\u0667':'7','\u0666':'6','\u0669':'9','\u0668':'8','\u0662':'2','\u0661':'1','\u0661\u0660':'10','\u0663':'3','2.60':'0','10.50':'0','10+':'10','\u0663+':'3','\u0661\u0660+':'10'})\n", "    df['bathrooms']=df['bathrooms'].astype('float')\n", "    df=df[['link','bathrooms','bedrooms','city','location1','location2','price','size','price_per_unit','type','unit','lat','lon','view','description','web_name','source','rent_sale']]\n", "    df['price_per_unit']=df['price']/df['size']\n", "    df.unit=df.unit.replace({'\u0634\u0642\u0642 \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0634\u0642\u0629','\u0634\u0642\u0642 \u0627\u064a\u062c\u0627\u0631 59\u0633\u0646\u0629':'\u0634\u0642\u0629','\u0634\u0642\u0642 \u0645\u0641\u0631\u0648\u0634\u0629 \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0634\u0642\u0629','\u063a\u0631\u0641 \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u063a\u0631\u0641\u0629','\u0641\u064a\u0644\u0627\u062a \u0648 \u0634\u0627\u0644\u064a\u0647\u0627\u062a':'\u0641\u064a\u0644\u0627','\u0645\u0634\u0631\u0648\u0639\u0627\u062a \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0645\u0634\u0631\u0648\u0639','\u0639\u0642\u0627\u0631\u0627\u062a \u062a\u062c\u0627\u0631\u064a\u0629':'\u0639\u0642\u0627\u0631','\u0645\u062d\u0644\u0627\u062a \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0645\u062d\u0644','\u0645\u0643\u0627\u062a\u0628 \u0648 \u0645\u0628\u0627\u0646\u0649 \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0645\u0643\u062a\u0628','\u0645\u062e\u0627\u0632\u0646 \u0648\u0627\u0631\u0627\u0636\u064a \u0644\u0644\u0627\u064a\u062c\u0627\u0631':'\u0627\u0631\u0636'})\n", "    df.loc[df.unit == '\u0639\u0642\u0627\u0631', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u062d\u0644', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u0643\u062a\u0628', 'type'] = \"\u0627\u062f\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0627\u0631\u0636', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0641\u064a\u0644\u0627', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0634\u0642\u0629', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u063a\u0631\u0641\u0629', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0645\u0634\u0631\u0648\u0639', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.drop(df[df['unit']=='\u0633\u064a\u0627\u0631\u0627\u062a \u0644\u0644\u0627\u064a\u062c\u0627\u0631'].index,inplace=True)\n", "    df.source='scraping'\n", "    return df.to_csv(r'tageer2_cleaning.csv',encoding ='utf-8-sig',index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def olx_scraping():\n", "    lst=[]                  #list of data\n", "#list of all links\n", "    links=[]\n", "    num_page=1\n", "    while True:\n", "        #header for the web site\n", "        headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n", "        #url for the website\n", "        url=f'https://www.olx.com.eg/properties/?page={num_page}'\n", "        print(url)\n", "        #connecting to web site\n", "        result = requests.get(url, headers=headers)\n", "        #content of the web site\n", "        src = result.content\n", "        #html code for the web site\n", "        soup = BeautifulSoup(src, \"lxml\")\n", "        #scraping links in each link\n", "        links_in_page=soup.find_all('div',{'class':'_41d2b9f3'})\n", "        #print(links_in_page)\n", "        #scraping links for each url\n", "        for i in links_in_page:\n", "            l='https://www.olx.com.eg'+i.find('a').attrs['href']\n", "            links.append(l)\n", "        #condition which counter will stop\n", "        next_page=soup.find('div',{'title':'\u0627\u0644\u062a\u0627\u0644\u064a'}) \n", "        if next_page!= None:\n", "            num_page+=1\n", "        else:\n", "            print('done')\n", "            break\n", "    #scraping data from each link\n", "    for link in links:\n", "        d={}\n", "        d['link']=link\n", "        print(link)\n", "        d['web_name']='olx'\n", "        d['source']='scraping'\n", "        response=requests.get(link,headers=headers)\n", "        text=response.content\n", "        soup2=BeautifulSoup(text,'html.parser')\n", "        try:\n", "            d['rent_sale']=soup2.find_all('a',{'class':'_151bd34b'})[2].text.partition(' ')[-1]\n", "            print(soup2.find_all('a',{'class':'_151bd34b'})[2].text.partition(' ')[-1])\n", "        except:\n", "            d['rent_sale']=None\n", "            print(d['rent_sale'])\n", "        try:\n", "            d['description']=soup2.find('div',{'class':'_0f86855a'}).text.replace('\\n','').strip()\n", "            print(soup2.find('div',{'class':'_0f86855a'}).text)\n", "        except:\n", "            d['description']=None\n", "            print(d['description'])\n", "        try:        \n", "            d['city']=soup2.find('span',{'class':'_8918c0a8'}).text.partition(', ')[2].strip()\n", "            print(d['city'])\n", "        except:\n", "            d['city']=None\n", "            print(d['city'])\n", "        try:        \n", "            d['location2']=soup2.find('span',{'class':'_8918c0a8'}).text.partition(', ')[0].strip().partition('-')[0].strip()\n", "            print(d['location2'])\n", "        except:\n", "            d['location2']=None\n", "            print(d['location2'])\n", "        try:\n", "            location1=soup2.find('span',{'class':'_8918c0a8'}).text.partition(',')[0].partition('-')[2].strip()\n", "            if location1=='':\n", "                d['location1']=d['location2']\n", "                print(d['location1'])\n", "            else:\n", "                d['location1']=location1\n", "                print(d['location1'])\n", "        except:\n", "            d['location1']=None\n", "            print(d['location1'])\n", "        try:\n", "            table2=soup2.find('div',{'class':'_241b3b1e'})        #scraping the table for each link\n", "            table=table2.find_all('div',{'class':\"b44ca0b3\"})\n", "            d['bedrooms']=None\n", "            d['bathrooms']=None\n", "            for n in table:\n", "                 try:\n", "                    if n.find_all('span')[0].text=='\u0627\u0644\u062d\u0645\u0627\u0645\u0627\u062a':\n", "                        d['bathrooms']=n.find_all('span')[1].text\n", "                        print(n.find_all('span')[1].text)\n", "                 except:\n", "                    d['bathrooms']=None\n", "                    print(None)\n", "                 try:\n", "                    if n.find_all('span')[0].text=='\u063a\u0631\u0641 \u0646\u0648\u0645':\n", "                        d['bedrooms']=n.find_all('span')[1].text\n", "                        print(n.find_all('span')[1].text)\n", "                 except:\n", "                    d['bedrooms']=None\n", "                    print(None)\n", "                 try:\n", "                    if n.find_all('span')[0].text=='\u0627\u0644\u0646\u0648\u0639':\n", "                        d['unit']=n.find_all('span')[1].text\n", "                        print(n.find_all('span')[1].text)\n", "                 except:\n", "                    d['unit']=None\n", "                    print(None)\n", "                 try:\n", "                    if n.find_all('span')[0].text=='\u0627\u0644\u0645\u0633\u0627\u062d\u0629 (\u0645\u0662)':\n", "                        d['size']=n.find_all('span')[1].text\n", "                        print(n.find_all('span')[1].text)\n", "                 except:\n", "                    d['size']=None\n", "                    print(None)\n", "                 try:\n", "                    if n.find_all('span')[0].text=='\u0627\u0644\u0633\u0639\u0631':\n", "                        d['price']=n.find_all('span')[1].text\n", "                        print(n.find_all('span')[1].text)\n", "                 except:\n", "                    d['price']=None\n", "                    print(None)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["        except:\n", "            pass\n", "        lst.append(d)\n", "        df=pd.DataFrame(lst)#exporting data to csv file\n", "        df.to_csv(r'olx_scraping.csv',encoding ='utf-8-sig',index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def olx_cleaning():\n", "    df=pd.read_csv(r'olx_scraping.csv',index_col=False)\n", "    df['all-location'] = df['location2'] + \",\" +  df['city']+','+'\u0645\u0635\u0631'\n", "    df['lat'] = \"\"\n", "    df['lon'] = \"\"\n", "    geolocator = Nominatim(user_agent=\"myApp\")\n", "    for i in df.index:\n", "        try:\n", "            #tries fetch address from geopy\n", "            location = geolocator.geocode(df['all-location'][i])\n\n", "            #append lat/long to column using dataframe location\n", "            df.loc[i,'lat'] = location.latitude\n", "            df.loc[i,'lon'] = location.longitude\n", "        except:\n", "            #catches exception for the case where no value is returned\n", "            #appends null value to column\n", "            df.loc[i,'lat'] = \"\"\n", "            df.loc[i,'lon'] = \"\"\n", "    del df['all-location']\n", "    #creating some columns\n", "    df['view']=''\n", "    df['type']=''\n", "    df['price_per_unit']=''\n", "    #replacing values\n", "    df.bedrooms=df.bedrooms.replace('10+','10')\n", "    #converting data types\n", "    df['bedrooms']=df['bedrooms'].astype('float')\n", "    df.bathrooms=df.bathrooms.replace('10+','10')\n", "    df['bathrooms']=df['bathrooms'].astype('float')\n", "    df.price=df.price.str.replace(',','')\n", "    df['price']=df['price'].astype('float')\n", "    #calculating price_per_unit column\n", "    df.price_per_unit=df.price/df.size\n", "    df.rent_sale=df.rent_sale.str.replace('\u062a\u062c\u0627\u0631\u0649','').str.replace('\u0645\u0635\u0627\u064a\u0641','').str.replace('\u0648 \u062f\u0648\u0628\u0644\u0643\u0633','').str.replace('\u0648 \u0623\u0631\u0627\u0636\u0649','\u0628\u064a\u0639').str.strip()\n", "    df.rent_sale=df.rent_sale.str.replace('\u0644\u0644\u0625\u064a\u062c\u0627\u0631','\u0627\u064a\u062c\u0627\u0631').str.replace('\u0644\u0644\u0628\u064a\u0639','\u0628\u064a\u0639').str.strip()\n", "    df.unit=df.unit.replace({'\u0641\u064a\u0644\u0627 \u0645\u0646\u0641\u0635\u0644\u0629':'\u0641\u064a\u0644\u0627'})\n", "    df.loc[df.unit == '\u0645\u062d\u0644', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u0628\u0646\u0649 \u062a\u062c\u0627\u0631\u0649 \u0643\u0627\u0645\u0644', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u0637\u0639\u0645 \u0648 \u0643\u0627\u0641\u064a\u0647', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0632\u0631\u0627\u0639\u064a\u0629', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u062a\u062c\u0627\u0631\u064a\u0629', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u062c\u0631\u0627\u062c', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u0635\u0646\u0639', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0644\u0643\u0644 \u0627\u0644\u0623\u063a\u0631\u0627\u0636', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0645\u062e\u0632\u0646', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0633\u062a\u0648\u062f\u064a\u0648', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0639\u064a\u0627\u062f\u0629', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0623\u062e\u0631\u0649', 'type'] = \"\u062a\u062c\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0634\u0642\u0629', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0645\u0643\u062a\u0628', 'type'] = \"\u0627\u062f\u0627\u0631\u064a\"\n", "    df.loc[df.unit == '\u0641\u064a\u0644\u0627', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u062f\u0648\u0628\u0644\u0643\u0633', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0634\u0627\u0644\u064a\u0647', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0633\u0643\u0646\u064a\u0629', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u063a\u0631\u0641\u0629', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u062a\u0627\u0648\u0646 \u0647\u0627\u0648\u0633', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u0628\u0646\u062a\u0647\u0627\u0648\u0633', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    df.loc[df.unit == '\u062a\u0648\u064a\u0646 \u0647\u0627\u0648\u0633', 'type'] = \"\u0633\u0643\u0646\u064a\"\n", "    #droping null values for some columns\n", "    df=df.dropna(subset=['lat'])\n", "    df=df.dropna(subset=['city'])\n", "    df=df[['link','bathrooms','bedrooms','city','location1','location2','price','size','price_per_unit','type','unit','lat','lon','view','description','web_name','source','rent_sale']]\n", "    return df.to_csv(r'olx_cleaning.csv',encoding ='utf-8-sig',index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[12]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["pensooq call----------------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Opensooq_scraping()\n", "Opensooq_clean()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Semsar_scraping()\n", "Semsar_cleaning()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tageer_scraping()\n", "tageer_cleaning()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["olx_scraping()\n", "olx_cleaning()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def concat():\n", "    file1=pd.read_csv(\"Opeensooq_Clean2.csv\")\n", "    file2=pd.read_csv(\"Semsar_clean.csv\")\n", "    file3=pd.read_csv(\"tageer2_cleaning.csv\")\n", "    file4=pd.read_csv(\"olx_cleaning.csv\")\n", "    \n", "    df=pd.concat([file1,file2,file3,file4],axis=0)\n", "    df.to_csv(\"all_data.csv\",encoding=\"utf-8-sig\",index=False)\n", "    print(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["concat()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}